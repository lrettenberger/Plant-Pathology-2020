{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "IMAGE_PATH = \"./data_source/images/\"\n",
    "TEST_PATH = \"./data_source/test.csv\"\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "SUB_PATH = \"./data_source/sample_submission.csv\"\n",
    "sub = pd.read_csv(SUB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import keras\n",
    "from efficientnet.keras import EfficientNetB6\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import DenseNet121,DenseNet201\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential , Model\n",
    "from tensorflow.keras.layers import Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam , RMSprop \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n",
    "\n",
    "img_size = 299\n",
    "\n",
    "def get_model(model_name) :\n",
    "    if model_name == 'MobileNet' :\n",
    "        base_model=MobileNet(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n",
    "    elif model_name == 'VGG16' : \n",
    "        base_model=vgg16.VGG16(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n",
    "    elif model_name == 'DenseNet' :\n",
    "        base_model = DenseNet121(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n",
    "    elif model_name == 'DenseNet201' :\n",
    "        base_model = DenseNet201(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n",
    "    elif model_name == 'Inception' :\n",
    "        base_model=InceptionV3(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n",
    "    elif model_name == 'ResNet' :\n",
    "        base_model = ResNet50(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n",
    "    elif model_name == 'EfficientNetB61' or model_name == 'EfficientNetB62' :\n",
    "        base_model = EfficientNetB6(weights=\"imagenet\",include_top=False,input_shape=(img_size,img_size,3))\n",
    "        model = keras.Sequential([base_model,keras.layers.GlobalAveragePooling2D(),keras.layers.Dense(4, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        return model\n",
    "    elif model_name == 'BinaryModelB6':\n",
    "        base_model = EfficientNetB6(input_shape=(img_size,img_size,3), include_top=False, weights=\"imagenet\")\n",
    "        model = keras.Sequential([base_model,keras.layers.GlobalAveragePooling2D(),keras.layers.Dense(1, activation='sigmoid')])\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        return model\n",
    "    x=base_model.output\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "    x=Dense(512,activation='relu')(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    x=Dense(256,activation='relu')(x) #dense layer 2\n",
    "    preds=Dense(4,activation='softmax')(x)\n",
    "    model=Model(inputs=base_model.input,outputs=preds,name=model_name)\n",
    "    model.compile(optimizer='Adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras_applications\\mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.5125 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5375 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5125 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5375 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "#nets = ['MobileNet','DenseNet','Inception','ResNet','EfficientNetB61','EfficientNetB62']\n",
    "#bin_nets = ['BinaryModelB6']\n",
    "nets = ['MobileNet','EfficientNetB61','EfficientNetB62','Inception','ResNet','DenseNet201']\n",
    "bin_nets = ['BinaryModelB6']\n",
    "CHECKPOINTS_BASE = 'checkpoints'\n",
    "weights = {\n",
    "    'MobileNet': '%s/MobileNet/best_model_val_loss.h5'%CHECKPOINTS_BASE,\n",
    "    'DenseNet': '%s/DenseNet/best_model_val_loss_0.18127.h5'%CHECKPOINTS_BASE,\n",
    "    'DenseNet201': '%s/DenseNet201/best_model_val_loss.h5'%CHECKPOINTS_BASE,\n",
    "    'Inception': '%s/Inception/best_model_val_loss.h5'%CHECKPOINTS_BASE,\n",
    "    'ResNet': '%s/ResNet/best_model_val_loss.h5'%CHECKPOINTS_BASE,\n",
    "    'EfficientNetB61': '%s/effnet/best_model.hdf5'%CHECKPOINTS_BASE,\n",
    "    'EfficientNetB62': '%s/effnet/best_model_1.8477e-06.hdf5'%CHECKPOINTS_BASE,\n",
    "    'BinaryModelB6': 'two_classes/zweiter/best_model_f1.hdf5'}\n",
    "\n",
    "models = []\n",
    "for net in nets:\n",
    "    model = get_model(net)\n",
    "    model.load_weights(weights.get(net))\n",
    "    models.append(model)\n",
    "\n",
    "bin_models = []\n",
    "bin_models.append(get_model(bin_nets[0]))\n",
    "bin_models[0].load_weights(weights.get(bin_nets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import image\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "size = (299,299)\n",
    "class_map = {'healthy':0,'multiple_diseases':1,'rust':2,'scab':3}\n",
    "\n",
    "def load_image(image_id,size):  \n",
    "    file_path = image_id\n",
    "    with Image.open(file_path) as image:\n",
    "        image = image.resize(size)\n",
    "    return np.asarray(image)\n",
    "\n",
    "import glob\n",
    "for img_path in glob.glob(\"data/Test/*/*.jpg\"):\n",
    "    Y_train.append(class_map.get(img_path.split('\\\\')[1]))\n",
    "    X_train.append(load_image(img_path,size) / 255.)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_preds(p,indices):\n",
    "    preds = models[0].predict(p)\n",
    "    preds.fill(0)\n",
    "    for i in range(len(models)):\n",
    "        if i in indices:\n",
    "            preds = preds + models[i].predict(p)\n",
    "    preds = preds / len(indices)\n",
    "    preds_classes = []\n",
    "    for pred in preds:\n",
    "        preds_classes.append(np.argmax(pred))\n",
    "    return preds_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from IPython.display import clear_output\n",
    "\n",
    "network_indices = [0,1,2,3,4,5]\n",
    "ensemble_scores = {}\n",
    "\n",
    "counter = 0\n",
    "end = 63\n",
    "for k in range(1,len(network_indices)+1):\n",
    "    combs = list(combinations(network_indices,k))\n",
    "    for comb in combs:\n",
    "        preds = average_preds(X_train,list(comb))\n",
    "        error = sum(preds != Y_train)\n",
    "        ensemble_scores[comb] = error\n",
    "        counter+=1\n",
    "        print('%.2f'%((counter/end)))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 5,\n",
       " (1, 2): 6,\n",
       " (2,): 7,\n",
       " (0, 4): 7,\n",
       " (1, 3): 7,\n",
       " (1, 4): 7,\n",
       " (1, 5): 7,\n",
       " (0, 1, 2): 7,\n",
       " (1, 2, 3): 7,\n",
       " (1, 2, 4): 7,\n",
       " (1, 2, 5): 7,\n",
       " (0, 1, 2, 3): 7,\n",
       " (0, 1, 2, 5): 7,\n",
       " (1, 2, 3, 4): 7,\n",
       " (1, 2, 3, 5): 7,\n",
       " (0, 1, 2, 3, 4): 7,\n",
       " (1,): 8,\n",
       " (0, 2): 8,\n",
       " (2, 3): 8,\n",
       " (2, 5): 8,\n",
       " (0, 1, 3): 8,\n",
       " (0, 1, 4): 8,\n",
       " (0, 1, 5): 8,\n",
       " (0, 2, 5): 8,\n",
       " (0, 3, 4): 8,\n",
       " (0, 4, 5): 8,\n",
       " (1, 3, 4): 8,\n",
       " (2, 3, 4): 8,\n",
       " (0, 1, 3, 5): 8,\n",
       " (0, 1, 2, 3, 5): 8,\n",
       " (0, 1, 2, 4, 5): 8,\n",
       " (1, 2, 3, 4, 5): 8,\n",
       " (0,): 9,\n",
       " (0, 3): 9,\n",
       " (2, 4): 9,\n",
       " (0, 2, 3): 9,\n",
       " (0, 2, 4): 9,\n",
       " (1, 4, 5): 9,\n",
       " (2, 4, 5): 9,\n",
       " (0, 1, 2, 4): 9,\n",
       " (0, 1, 3, 4): 9,\n",
       " (0, 1, 4, 5): 9,\n",
       " (0, 2, 3, 5): 9,\n",
       " (0, 3, 4, 5): 9,\n",
       " (1, 2, 4, 5): 9,\n",
       " (2, 3, 4, 5): 9,\n",
       " (0, 1, 3, 4, 5): 9,\n",
       " (0, 1, 2, 3, 4, 5): 9,\n",
       " (0, 5): 10,\n",
       " (3, 4): 10,\n",
       " (4, 5): 10,\n",
       " (0, 3, 5): 10,\n",
       " (1, 3, 5): 10,\n",
       " (2, 3, 5): 10,\n",
       " (3, 4, 5): 10,\n",
       " (0, 2, 3, 4): 10,\n",
       " (0, 2, 4, 5): 10,\n",
       " (1, 3, 4, 5): 10,\n",
       " (0, 2, 3, 4, 5): 10,\n",
       " (3,): 12,\n",
       " (5,): 12,\n",
       " (3, 5): 12,\n",
       " (4,): 13}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_scores_sorted = ensemble_scores\n",
    "\n",
    "{k: v for k, v in sorted(ensemble_scores_sorted.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Images: 99.945085%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import image\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(image_id,size):  \n",
    "    file_path = image_id + \".jpg\"\n",
    "    with Image.open(IMAGE_PATH + file_path) as image:\n",
    "        image = image.resize(size)\n",
    "    return np.asarray(image)\n",
    "\n",
    "IMAGE_PATH = \"./data_source/images/\"\n",
    "TEST_PATH = \"./data_source/test.csv\"\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "\n",
    "\n",
    "size = (image_size[1], image_size[0])\n",
    "\n",
    "# Test data of the challenge (no y values)\n",
    "print(\"Loading Test Images\")\n",
    "X_test = []\n",
    "progress = 0\n",
    "for image_id in test_data[\"image_id\"]:\n",
    "    X_test.append(load_image(image_id,size) / 255.)\n",
    "    print('Loading Test Images: %f%%'%((progress/len(test_data[\"image_id\"])*100)))\n",
    "    progress+=1\n",
    "    clear_output(wait=True)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "class_map = {'healthy':0,'multiple_diseases':1,'rust':2,'scab':3}\n",
    "\n",
    "def load_image(image_id,size):  \n",
    "    file_path = image_id\n",
    "    with Image.open(file_path) as image:\n",
    "        image = image.resize(size)\n",
    "    return np.asarray(image)\n",
    "\n",
    "import glob\n",
    "for img_path in glob.glob(\"data/Test/*/*.jpg\"):\n",
    "    Y_train.append(class_map.get(img_path.split('\\\\')[1]))\n",
    "    X_train.append(load_image(img_path,size) / 255.)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 4s 25ms/step\n",
      "182/182 [==============================] - 4s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[48,  0,  0,  0],\n",
       "       [ 1,  8,  2,  1],\n",
       "       [ 0,  1, 61,  0],\n",
       "       [ 0,  1,  0, 59]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_preds(p):\n",
    "    preds = models[0].predict(p,verbose=1)\n",
    "    for i in range(1,len(models)):\n",
    "        preds = preds + models[i].predict(p,verbose=1)\n",
    "    preds = preds / len(models)\n",
    "    return preds\n",
    "\n",
    "def ensemble(preds,mutli_prop,multi_importance = 0.8):\n",
    "    for i in range(len(preds)):\n",
    "        pred = preds[i]\n",
    "        multi = mutli_prop[i]\n",
    "        #if multi > 0.8:\n",
    "        for j in range(len(pred)):\n",
    "            if j==1:\n",
    "                preds[i][j] = (preds[i][j]*(1-multi_importance)+(multi*multi_importance))\n",
    "            else:\n",
    "                preds[i][j] = (preds[i][j]*(1-multi_importance)+(1-multi)*multi_importance)\n",
    "    return preds\n",
    "preds = ensemble(average_preds(X_train),bin_models[0].predict(X_train),multi_importance=0)\n",
    "preds_single = []\n",
    "for pred in preds:\n",
    "    preds_single.append(np.argmax(pred))\n",
    "\n",
    "confusion_matrix(Y_train,preds_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 4s 25ms/step\n",
      "182/182 [==============================] - 4s 25ms/step\n",
      "--------------\n",
      "Multi_importance: 0.510000\n",
      "Wrongs: 1\n",
      "[[48  0  0  0]\n",
      " [ 0 12  0  0]\n",
      " [ 0  0 62  0]\n",
      " [ 0  1  0 59]]\n",
      "--------------\n",
      "\n",
      "182/182 [==============================] - 4s 25ms/step\n",
      "182/182 [==============================] - 4s 25ms/step\n",
      "--------------\n",
      "Multi_importance: 0.512600\n",
      "Wrongs: 1\n",
      "[[48  0  0  0]\n",
      " [ 0 12  0  0]\n",
      " [ 0  0 62  0]\n",
      " [ 0  1  0 59]]\n",
      "--------------\n",
      "\n",
      "182/182 [==============================] - 4s 25ms/step\n",
      "182/182 [==============================] - 4s 25ms/step\n",
      "--------------\n",
      "Multi_importance: 0.515200\n",
      "Wrongs: 1\n",
      "[[48  0  0  0]\n",
      " [ 0 12  0  0]\n",
      " [ 0  0 62  0]\n",
      " [ 0  1  0 59]]\n",
      "--------------\n",
      "\n",
      "182/182 [==============================] - 4s 25ms/step\n",
      " 32/182 [====>.........................] - ETA: 3s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-43400f140649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mwrongs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mmulti_importance\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0.640000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_preds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbin_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmulti_importance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_importance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpreds_single\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-a98a258db17b>\u001b[0m in \u001b[0;36maverage_preds\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3566\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3567\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3568\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3569\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "multi_importance = 0.575\n",
    "wrongs = []\n",
    "while multi_importance<=0.640000:\n",
    "    preds = ensemble(average_preds(X_train),bin_models[0].predict(X_train),multi_importance=multi_importance)\n",
    "    preds_single = []\n",
    "    for pred in preds:\n",
    "        preds_single.append(np.argmax(pred))\n",
    "    wrongs.append(sum(Y_train!=preds_single))\n",
    "    print('--------------')\n",
    "    print('Multi_importance: %f'%multi_importance)\n",
    "    print('Wrongs: %d'%sum(Y_train!=preds_single))\n",
    "    print(confusion_matrix(Y_train,preds_single))\n",
    "    print('--------------')\n",
    "    print()\n",
    "    multi_importance+=0.0026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Images: 99.945085%\n"
     ]
    }
   ],
   "source": [
    "def load_image(image_id,size):  \n",
    "    file_path = image_id + \".jpg\"\n",
    "    with Image.open(IMAGE_PATH + file_path) as image:\n",
    "        image = image.resize(size)\n",
    "    return np.asarray(image)\n",
    "\n",
    "\n",
    "print(\"Loading Test Images\")\n",
    "X_test = []\n",
    "progress = 0\n",
    "for image_id in test_data[\"image_id\"]:\n",
    "    X_test.append(load_image(image_id,size) / 255.)\n",
    "    print('Loading Test Images: %f%%'%((progress/len(test_data[\"image_id\"])*100)))\n",
    "    progress+=1\n",
    "    clear_output(wait=True)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_preds(p,indices):\n",
    "    preds = models[0].predict(p)\n",
    "    preds.fill(0)\n",
    "    for i in range(len(models)):\n",
    "        if i in indices:\n",
    "            preds = preds + models[i].predict(p)\n",
    "    preds = preds / len(indices)\n",
    "    return preds\n",
    "\n",
    "def ensemble(preds,mutli_prop):\n",
    "    multi_importance = 0.8\n",
    "    for i in range(len(preds)):\n",
    "        pred = preds[i]\n",
    "        multi = mutli_prop[i]\n",
    "        if multi > 0.8:\n",
    "            for j in range(len(pred)):\n",
    "                if j==1:\n",
    "                    preds[i][j] = (preds[i][j]*(1-multi_importance)+(multi*multi_importance))\n",
    "                else:\n",
    "                    preds[i][j] = (preds[i][j]*(1-multi_importance)+(1-multi)*multi_importance)\n",
    "    return preds\n",
    "\n",
    "def LabelSmoothing(encodings , alpha=0.01):\n",
    "    K = encodings.shape[1]\n",
    "    y_ls = (1 - alpha) * encodings + alpha / K\n",
    "    return y_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.985056</td>\n",
       "      <td>0.004975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.007106</td>\n",
       "      <td>0.982931</td>\n",
       "      <td>0.004975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.984730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>0.985048</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.004976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>0.972535</td>\n",
       "      <td>0.004977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id   healthy  multiple_diseases      rust      scab\n",
       "0   Test_0  0.004976           0.004993  0.985056  0.004975\n",
       "1   Test_1  0.004988           0.007106  0.982931  0.004975\n",
       "2   Test_2  0.004978           0.005317  0.004975  0.984730\n",
       "3   Test_3  0.985048           0.005000  0.004976  0.004976\n",
       "4   Test_4  0.004982           0.017506  0.972535  0.004977"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_importance = 0\n",
    "#multi_importance = 0.575\n",
    "probs_efnns = average_preds(X_test,[2,3])\n",
    "probs_efnns = LabelSmoothing(probs_efnns)\n",
    "sub.loc[:, 'healthy':] = probs_efnns\n",
    "sub.to_csv('./submission_efnns.csv', index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
