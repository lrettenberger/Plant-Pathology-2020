{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "IMAGE_PATH = \"./data_source/images/\"\n",
    "TEST_PATH = \"./data_source/test.csv\"\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "SUB_PATH = \"./data_source/sample_submission.csv\"\n",
    "sub = pd.read_csv(SUB_PATH)\n",
    "from Model import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = ['MobileNet','EfficientNetB61','EfficientNetB62','Inception','ResNet','DenseNet201']\n",
    "bin_nets = ['BinaryModelB6']\n",
    "CHECKPOINTS_BASE = 'checkpoints'\n",
    "weights = {\n",
    "    'MobileNet': '%s/MobileNet/best_model_val_loss.h5'%CHECKPOINTS_BASE,\n",
    "    'DenseNet': '%s/DenseNet/best_model_val_loss_0.18127.h5'%CHECKPOINTS_BASE,\n",
    "    'DenseNet201': '%s/DenseNet201/best_model_val_loss.h5'%CHECKPOINTS_BASE,\n",
    "    'Inception': '%s/Inception/best_model_val_loss.h5'%CHECKPOINTS_BASE,\n",
    "    'ResNet': '%s/ResNet/best_model_val_loss.h5'%CHECKPOINTS_BASE,\n",
    "    'EfficientNetB61': '%s/effnet/best_model.hdf5'%CHECKPOINTS_BASE,\n",
    "    'EfficientNetB62': '%s/effnet/best_model_1.8477e-06.hdf5'%CHECKPOINTS_BASE,\n",
    "    'BinaryModelB6': '%s/b6/best_model_f1.hdf5'%CHECKPOINTS_BASE}\n",
    "\n",
    "models = []\n",
    "for net in nets:\n",
    "    model = get_model(net)\n",
    "    model.load_weights(weights.get(net))\n",
    "    models.append(model)\n",
    "\n",
    "bin_models = []\n",
    "bin_models.append(get_model(bin_nets[0]))\n",
    "bin_models[0].load_weights(weights.get(bin_nets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import image\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "size = (299,299)\n",
    "class_map = {'healthy':0,'multiple_diseases':1,'rust':2,'scab':3}\n",
    "\n",
    "def load_image(image_id,size):  \n",
    "    file_path = image_id\n",
    "    with Image.open(file_path) as image:\n",
    "        image = image.resize(size)\n",
    "    return np.asarray(image)\n",
    "\n",
    "import glob\n",
    "for img_path in glob.glob(\"data/Test/*/*.jpg\"):\n",
    "    Y_train.append(class_map.get(img_path.split('\\\\')[1]))\n",
    "    X_train.append(load_image(img_path,size) / 255.)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_preds(p,indices):\n",
    "    preds = models[0].predict(p)\n",
    "    preds.fill(0)\n",
    "    for i in range(len(models)):\n",
    "        if i in indices:\n",
    "            preds = preds + models[i].predict(p)\n",
    "    preds = preds / len(indices)\n",
    "    preds_classes = []\n",
    "    for pred in preds:\n",
    "        preds_classes.append(np.argmax(pred))\n",
    "    return preds_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import image\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(image_id,size):  \n",
    "    file_path = image_id + \".jpg\"\n",
    "    with Image.open(IMAGE_PATH + file_path) as image:\n",
    "        image = image.resize(size)\n",
    "    return np.asarray(image)\n",
    "\n",
    "IMAGE_PATH = \"./data_source/images/\"\n",
    "TEST_PATH = \"./data_source/test.csv\"\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "\n",
    "\n",
    "size = (image_size[1], image_size[0])\n",
    "\n",
    "# Test data of the challenge (no y values)\n",
    "print(\"Loading Test Images\")\n",
    "X_test = []\n",
    "progress = 0\n",
    "for image_id in test_data[\"image_id\"]:\n",
    "    X_test.append(load_image(image_id,size) / 255.)\n",
    "    print('Loading Test Images: %f%%'%((progress/len(test_data[\"image_id\"])*100)))\n",
    "    progress+=1\n",
    "    clear_output(wait=True)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "class_map = {'healthy':0,'multiple_diseases':1,'rust':2,'scab':3}\n",
    "\n",
    "def load_image(image_id,size):  \n",
    "    file_path = image_id\n",
    "    with Image.open(file_path) as image:\n",
    "        image = image.resize(size)\n",
    "    return np.asarray(image)\n",
    "\n",
    "import glob\n",
    "for img_path in glob.glob(\"data/Test/*/*.jpg\"):\n",
    "    Y_train.append(class_map.get(img_path.split('\\\\')[1]))\n",
    "    X_train.append(load_image(img_path,size) / 255.)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_preds(p):\n",
    "    preds = models[0].predict(p,verbose=1)\n",
    "    for i in range(1,len(models)):\n",
    "        preds = preds + models[i].predict(p,verbose=1)\n",
    "    preds = preds / len(models)\n",
    "    return preds\n",
    "\n",
    "def ensemble(preds,mutli_prop,multi_importance = 0.8):\n",
    "    for i in range(len(preds)):\n",
    "        pred = preds[i]\n",
    "        multi = mutli_prop[i]\n",
    "        #if multi > 0.8:\n",
    "        for j in range(len(pred)):\n",
    "            if j==1:\n",
    "                preds[i][j] = (preds[i][j]*(1-multi_importance)+(multi*multi_importance))\n",
    "            else:\n",
    "                preds[i][j] = (preds[i][j]*(1-multi_importance)+(1-multi)*multi_importance)\n",
    "    return preds\n",
    "preds = ensemble(average_preds(X_train),bin_models[0].predict(X_train),multi_importance=0)\n",
    "preds_single = []\n",
    "for pred in preds:\n",
    "    preds_single.append(np.argmax(pred))\n",
    "\n",
    "confusion_matrix(Y_train,preds_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_id,size):  \n",
    "    file_path = image_id + \".jpg\"\n",
    "    with Image.open(IMAGE_PATH + file_path) as image:\n",
    "        image = image.resize(size)\n",
    "    return np.asarray(image)\n",
    "\n",
    "\n",
    "print(\"Loading Test Images\")\n",
    "X_test = []\n",
    "progress = 0\n",
    "for image_id in test_data[\"image_id\"]:\n",
    "    X_test.append(load_image(image_id,size) / 255.)\n",
    "    print('Loading Test Images: %f%%'%((progress/len(test_data[\"image_id\"])*100)))\n",
    "    progress+=1\n",
    "    clear_output(wait=True)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_preds(p,indices):\n",
    "    preds = models[0].predict(p)\n",
    "    preds.fill(0)\n",
    "    for i in range(len(models)):\n",
    "        if i in indices:\n",
    "            preds = preds + models[i].predict(p)\n",
    "    preds = preds / len(indices)\n",
    "    return preds\n",
    "\n",
    "def ensemble(preds,mutli_prop):\n",
    "    multi_importance = 0.8\n",
    "    for i in range(len(preds)):\n",
    "        pred = preds[i]\n",
    "        multi = mutli_prop[i]\n",
    "        if multi > 0.8:\n",
    "            for j in range(len(pred)):\n",
    "                if j==1:\n",
    "                    preds[i][j] = (preds[i][j]*(1-multi_importance)+(multi*multi_importance))\n",
    "                else:\n",
    "                    preds[i][j] = (preds[i][j]*(1-multi_importance)+(1-multi)*multi_importance)\n",
    "    return preds\n",
    "\n",
    "def LabelSmoothing(encodings , alpha=0.01):\n",
    "    K = encodings.shape[1]\n",
    "    y_ls = (1 - alpha) * encodings + alpha / K\n",
    "    return y_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_importance = 0.575\n",
    "probs_efnns = average_preds(X_test,[2,3])\n",
    "probs_efnns = LabelSmoothing(probs_efnns)\n",
    "sub.loc[:, 'healthy':] = probs_efnns\n",
    "sub.to_csv('./submission_efnns.csv', index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
